---
author: Zaki Mughal
date: 2023-09-14
---

# Plan

## Conversion from dataset to RDF representation

- *Data are already in RDF representation*: Then use the RDF data directly.
  The data are likely already modelled and use existing ontologies.
- *Data are in a tabular or relational representation*:
    * This requires a mapping for columns for each row.
    * Direct mappings might be possible. By direct mappings, I mean:
        + Choose a set of columns as the keys which uniquely identify a row.
            + Turn this set of columns into an **subject** IRI `<S_r>` for each
              row using a template.
        + For the rest of the columns headers, assign an **predicate** IRI
          `<P_c>` to represent the relationship between `<S_r>` and the literal
          stored in that particular `(r,c)` cell.
    * Fully-automatic direct-mapping approach for relational data:
        + Specs: [A Direct Mapping of Relational Data to RDF](https://www.w3.org/TR/rdb-direct-mapping/)
        + Requires:
            + Depending on the software, the data need to be loaded into relational DB (MySQL, SQLite, PostgreSQL).
        + This is specifically made for relational databases.
        + Possibly works best when the relational DB has a [normalised schema](https://en.wikipedia.org/wiki/Database_normalization).
        + Thus this is probably only a first approximation for general
          tabular data which may not be normalised.
            + For example, the tabular data may need to be pivoted as the relationship
              between a subject and object may be stored in a cell as with
              [narrow data](https://en.wikipedia.org/wiki/Wide_and_narrow_data#Narrow).
        + *Recommendation*: if the data are already available as a DB dump (e.g.,
          MySQL dump), use this as the first step of a pipeline or first
          attempt at creating a graph because there is already a type of
          relational schema.
    * Semi-automatic direct mapping:
        + In this, every column is labelled with the IRI that the data represents.
        + This is also roughly the approach behind CSV on the Web (CSVW)
            + Specs: [GitHub - w3c/csvw: Documents produced by the CSV on the Web Working Group](https://github.com/w3c/csvw)
            + Requires:
                + Writing a JSON-LD file with the mapping.
            + Implementations:
                + [GitHub - ruby-rdf/rdf-tabular: Tabular Data RDF Reader and JSON serializer](https://github.com/ruby-rdf/rdf-tabular)
                    + *Note*: Recommend this approach of the two for this task as the metadata is out-of-band.
                + [GitHub - polyneme/csv-ld: A CSV-based Serialization for Linked Data](https://github.com/polyneme/csv-ld)
                    + Uses CSVW, but with in-band metadata, better for data producers (not us).
        + *Recommendation*: This might be easier for beginners to use as long
          as there is template. This template could even be generated by
          looking at the tabular data headers.
    * Non-direct mapping approach 1: R2RML
        + Specs:
            + [R2RML: RDB to RDF Mapping Language](https://www.w3.org/TR/r2rml/)
                + draft spec for extension: [RML-Core](https://kg-construct.github.io/rml-core/spec/docs/)
                + equivalent expressiveness as direct mapping: [GitHub - chrdebru/direct-mapping: Simulating a Direct Mapping with an R2RML mapping](https://github.com/chrdebru/direct-mapping)
        + Requires:
            + Writing a mapping in the form of a Turtle file.
        + Implementations:
            + [RML.io](https://rml.io/), [RMLio/rmlmapper-java](https://github.com/RMLio/rmlmapper-java)
                + Formats: Excel, CSV, JSON, etc.
                + Note: has some interesting work that could make using this
                  method simpler:
                    + Web interface for editing a simple YAML file called
                      [YARRRML](https://rml.io/yarrrml/), similar to
                      how CSVW requires editing a JSON-LD file <https://rml.io/yarrrml/matey/>.
                      Since it is in the browser, a user would develop the
                      mapping using a subset of the tabular data. To make things easier, it should
                      also have a template generator like suggested for CSVW.
            + [Morph-KGC](https://morph-kgc.readthedocs.io/en/latest/)
                + Formats: Excel, CSV, JSON, Parquet, etc.
                + Supports user-defined functions in Python.
            + Overview of others:
                + [Why RDF is struggling - the case of R2RML - Tomasz Pluskiewicz](https://t-code.pl/blog/2020/07/rdf-struggling-case-of-r2rml/)
                + [RDB2RDF Implementation Report](https://www.w3.org/TR/rdb2rdf-implementations/)
        + Drawbacks:
            + If the data are not normalised, creating that mapping is not easy.
              Also, does not currently support RDF container types (this is being developed)
            + While there are implementation-specific ways to do some
              manipulation using functions, the syntax for calling a function
              is complex.
            + R2RML is verbose even as Turtle.
    * Non-direct mapping approach 2: TARQL
        + Implementation: [Tarql: SPARQL for Tables – Tarql – SPARQL for Tables: Turn CSV into RDF using SPARQL syntax](https://tarql.github.io/)
        + Requires:
            + CSV file.
            + Writing a SPARQL query.
        + Example: [Converting CSV to RDF with Tarql](https://bobdc.com/blog/tarql/)
    * Non-direct mapping approach 3: SPARQL CONSTRUCT: SPARQL to rewrite graph.
        + Requires:
            + RDF data.
            + Writing a SPARQL query.
        + Approach: Take any of the outputs from the previous steps and use
          SPARQL query to create a new graph.
        + This can be used with any other previous method.
            + This can be used to go from one graph shape to another shape more easily.
            + Other RDF sources can be used to map literals to IRIs (either
              through a federated query or a local RDF file).
                + Example: mapping a species name string literal to a NCBI Taxonomy ID IRI.

*Recommendation*: Decision stump for which approach to use:

- Know SPARQL:
    * Yes: use (direct mapping or CSVW JSON-LD) + SPARQL construct
    * No: use CSVW JSON-LD or YARRRML generated from template, edit this, then hand
      this over to somebody who can refine using SPARQL.

*Recommendation*: Create a flow chart or table for which identifiers to use for
particular classes of entities. Biolink Model would be a good source for this.

## Local / ready-to-use knowledge graph

RDF graphs in the form of Turtle / N-Triples text formats are often distributed
in compressed form (e.g., `.ttl.gz`, `.nt.gz`) and these have wide support, but
they can still be large and need to be indexed by a database for fast querying.

An alternative is to use a binary format called [RDF HDT](https://www.rdfhdt.org/)
which compresses and indexes RDF data in a binary format.

The drawback is that while HDT has several implementations, it is not an
official W3C standard, but has been submitted to the W3C as
[Binary RDF Representation for Publication and Exchange (HDT) Submission](https://www.w3.org/submissions/2011/03/).

## Biobricks for graphs

- Q: What to call new bricks that represent local knowledge graphs?
    * A: "{dataset-name}-kg" ?

- *Data have an existing RDF representation*: Some datasets are already in RDF
  (they may also have a tabular representation that the data producer provides).
  In this case, create a new brick from the template that creates an RDF HDT
  file. Using the tabular representation is not necessary if the RDF already
  contains all the information.
- *Data only have a tabular representation*: Create a new brick that depends on
  the Parquet representation of the tabular data.

## Querying knowledge graphs

- With HDT, a single knowledge graph can be queried directly using both SPARQL
  and Gremlin graph query langauges:
    * Jena <https://www.rdfhdt.org/manual-of-hdt-integration-with-jena/>
    * Gremlin <https://github.com/rdfhdt/hdt-gremlin>
  This does not require decompression of the data.

- If a particular front-end must be used, then the RDF data can be loaded into
  an RDF compatible graph database without any need for additional schema
  creation.
    * A comparison of RDF graph databases is available [here](https://users.elis.ugent.be/~drdwitte/featurematrix.html).
        + Paper and code for above: [GitHub - drdwitte/benchmarking_biom_semantics: Research on benchmarking Triple Stores](https://github.com/drdwitte/benchmarking_biom_semantics)
    * Selected RDF graph databases / technologies
        + OpenLink Virtuoso
            + Bulk loading: <https://github.com/openlink/vos-docker-bulkload-example>.
              *Note*: I am already loading [ChEMBL-RDF](https://chembl.gitbook.io/chembl-interface-documentation/downloads)
              from the Turtle files this way. I have also the loader scripts
              for [PubChemRDF](https://pubchem.ncbi.nlm.nih.gov/docs/rdf) ready
              using the [recommended steps](https://pubchem.ncbi.nlm.nih.gov/docs/rdf-load).
        + Jena Fuseki
            + Can use HDT directly: [Manual of HDT integration with Jena – RDF HDT](https://www.rdfhdt.org/manual-of-hdt-integration-with-jena/)
- Perhaps the approach is to use Jena Fuseki as the SPARQL server and
  Virtuoso's tools to retrieve from it so that no transformation of the storage
  backend is needed.

## Existing Ontologies

### Toxicology specific

- Environmental conditions, treatments and exposures ontology (ECTO)
    * [GitHub - EnvironmentOntology/environmental-exposure-ontology: Modular environmental exposures ontology](https://github.com/EnvironmentOntology/environmental-exposure-ontology)
    * Part of OBO.
    * Composed of other ontologies including: ExO [GitHub - CTDbase/exposure-ontology: First pass at repo for Exo](https://github.com/CTDbase/exposure-ontology)
- OpenTox:
    * Described in paper [OpenTox predictive toxicology framework: toxicological ontology and semantic media wiki-based OpenToxipedia (2012)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3337268/)
    * Older resources:
        + [Toxicological Ontology — OpenTox](http://old.opentox.org/dev/Ontology/Toxicological%20Ontology)
        + [Ontology — OpenTox](http://old.opentox.org/dev/apis/api-1.2/Ontology)
    * Is now associated with [ToxBank | Data warehouse for toxicity data management](http://www.toxbank.net/):
        + [GitHub - ToxBank/toxbank-ontology](https://github.com/ToxBank/toxbank-ontology)
- ComptoxAI
    * Described in paper [Automating Predictive Toxicology Using ComptoxAI (2022)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9805296/)
    * [ComptoxAI OWL2 Ontology](https://comptox.ai/browse.html)

### Assays

- [BioAssay Ontology | The BioAssay Ontology (BAO) describes chemical biology screening assays and their results including high-throughput screening (HTS) data for the purpose of categorizing assays and data analysis.](http://bioassayontology.org/)
    * [GitHub - BioAssayOntology/BAO](https://github.com/BioAssayOntology/BAO)
    * Example terms:
        + [LD50](https://www.ebi.ac.uk/ols/ontologies/bao/terms?iri=http%3A%2F%2Fwww.bioassayontology.org%2Fbao%23BAO_0002117&lang=en&viewMode=All&siblings=true)

### Chemistry

- [GitHub - semanticchemistry/semanticchemistry: Chemical Information Ontology](https://github.com/semanticchemistry/semanticchemistry)
    * Example terms:
        + [CAS registry number](https://www.ebi.ac.uk/ols/ontologies/cheminf/terms?iri=http%3A%2F%2Fsemanticscience.org%2Fresource%2FCHEMINF_000446&lang=en&viewMode=All&siblings=true)

## Data QA

- Basic syntax checking of RDF output (already have script that does this using Jena).
- Metadata
    * Use [Describing Linked Datasets with the VoID Vocabulary](https://www.w3.org/TR/void/) in the brick. There is an extension of VoID to work with HDT.
    * Provenance Ontology: [PROV (Provenance) - Wikipedia](https://en.wikipedia.org/wiki/PROV_(Provenance)).
- Distribute example SPARQL queries in brick for both documentation and testing purposes.
    * If needed, a SPARQL query could be used to recover the original tabular data.
- Create SHACL constraints to validate the data: [Validating RDF data with SHACL](https://www.bobdc.com/blog/validating-rdf-data-with-shacl/).
    * Use [Linked Data Patterns](https://patterns.dataincubator.org/) to document what type of relations are being modelled.

## What I need to figure out

- Which identifiers to use when there are multiple identifiers:
    * I want to load the larger RDF datasets to find out if an identifier in one graph is available in another graph.
    * References:
        + [Mapping of chemical identifiers to DSSTox to enable data integration in the US-EPA CompTox Chemicals Dashboard | Science Inventory | US EPA](https://cfpub.epa.gov/si/si_public_record_report.cfm?Lab=NCCT&dirEntryId=344993)
        + [Consistency of systematic chemical identifiers within and between small-molecule databases - PMC](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3539895/)
    * Part of the QA process could be to make sure that all identifiers are available in another graph.
- For data with nullable columns, need to be careful as some of those could be
  meaningful.
    * First pass could drop rows that have nulls in particular columns.
    * Will need to create a schema to understand the dependency.
- Can bricks have a build dependency and "runtime" dependency?
    * As mentioned under identifier QA, it would be nice to be able to pull all
      the graphs that are the canonical source of the identifiers used within
      the local knowledge graph so that they can be immediately joined.

## Stretch goals

- Be aware of [ISO 20691 - Requirements for data formatting and description in the life sciences](https://fairsharing.org/3533) standard development.
- [The RDF Data Cube Vocabulary](https://www.w3.org/TR/vocab-data-cube/): might be useful for working with data with many dimensions (like OLAP).
    * Limitations: tooling does not seem to be standardised.
    * On [FAIRsharing](https://doi.org/10.25504/FAIRsharing.c3b573)
- Natural Language to SPARQL
    * <https://aksw.org/Projects/NeuralSPARQLMachines.html>
- Visualisation / exploration tools for RDF (in the browser)
    * [Virtuoso Faceted Browser](https://vos.openlinksw.com/owiki/wiki/VOS/VirtuosoFacetsWebService)
    * [Virtuoso Sponger](https://vos.openlinksw.com/owiki/wiki/VOS/VirtSponger)
    * [GitHub - LodLive/LodView: IRI dereferencer, RDF to HTML](https://github.com/LodLive/LodView)
    * There seem to be some tools at [Center on Knowledge Graphs](https://usc-isi-i2.github.io/home/).
